[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Algorithm Codex",
    "section": "",
    "text": "Preface\nWelcome to The Algorithm Codex.\nThis book is a repository of common algorithms used in all areas of computer science. It contains reference implementations in Python for many well-known (and some not-so-much) algorithms spanning from simple linear search to sorting, graphs, computational geometry, data structures, flow networks, game theory, number theory, optimization, and many other fields.\nWe wrote this book to serve as a complement for the main bibliography in a typical Computer Science major. You will not find comprehensive theory of algorithms in this book, or detailed analyses. However, we do present some basic intuitions into why most of the presented algorithms work and a back-of-the-envelope cost analysis whenever it makes sense.\nThe order in which algorithms are presented is our best attempt to build the most complex ideas on top of the simpler ones. We start with basic algorithms that everyone can understand and progressively move towards the more advanced.\nThe algorithms are presented in a literate programming format. The gist is that we combine code and prose in the best possible way to maximize understanding. Actually, the source code is generated from the book source, and not the other way around–that is what literate programming is, after all. Accompanying this book, you will find an open source repository with the exact implementations in this book.\nYou can read the book online at https://matcom.github.io/codex/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#content-of-the-book",
    "href": "index.html#content-of-the-book",
    "title": "The Algorithm Codex",
    "section": "Content of the Book",
    "text": "Content of the Book\nThe Algorithm Codex is organized into several major parts, designed to take you from foundational concepts to specialized domains and the limits of computation:\n\nSearching and Sorting: We establish the core intuitions of algorithmic efficiency by exploring how to find and organize data in linear and logarithmic time.\nFundamental Data Structures: We implement essential abstractions—including linked lists, stacks, queues, and hash tables—that serve as the building blocks for more complex systems.\nTrees: This part covers hierarchical data, from binary search trees to self-balancing structures and specialized variants like heaps and tries.\nString Algorithms: We focus on pattern matching and text processing, covering algorithms from exact matching (KMP, Boyer-Moore) to advanced suffix structures.\nGraphs: A significant section dedicated to relational data, covering traversals, shortest paths, spanning trees, and flow networks.\nDynamic Programming and Greedy Algorithms: We delve into powerful paradigms for solving optimization problems by exploiting subproblem structure and local optimality.\nSpecialized Domains: We explore deep subregions of Computer Science, including computational geometry, number theory, and game theory.\nAdvanced Complexity: The book concludes with the frontiers of computation, exploring NP-completeness, approximation algorithms, and randomized approaches.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-coding-style",
    "href": "index.html#about-the-coding-style",
    "title": "The Algorithm Codex",
    "section": "About the coding style",
    "text": "About the coding style\nThe code in this book is written in Python 3, specifically the 3.13 version.\nWe make extensive use of Python’s generic syntax to write clean but fully typed methods that leverage the best and most modern practices in software development. Other than that, the code is often written in the simplest possible way that works. We don’t make unnecessary optimizations like taking bounds out of a loop. On the other hand, our code is optimized in the algorithmic sense; it is fast because it exploits the inherent structure of the problem.\nSince most of our code is pure, functional algorithms, we often rely on public, plain Python functions. We thus have very few classes, and the ones we have are very simple, often nothing but data stores. However, we do make heavy use of protocols and abstract classes, especially those in the Python standard library like sequences, maps, and queues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#support-the-algorithm-codex",
    "href": "index.html#support-the-algorithm-codex",
    "title": "The Algorithm Codex",
    "section": "Support The Algorithm Codex",
    "text": "Support The Algorithm Codex\nThis book is free, as in free beer and free speech, and it will always be.\nThe book content is licensed CC-BY-NC-SA, that means you are free to share the book in any format (HTML, ePUB, PDF) with anyone, and produce any derivatives you want, as long as you also share those freely for posterity.\nThe source code is licensed MIT, and thus you can use the algorithms implemented here for anything, including classes, academic work, but also writing commercial software.\nThe only thing you cannot do is resell the book itself or any derivative work like lecture notes, translations, etc.\nIf you want to support this effort, the best way to do is to buy the official PDF.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#stay-in-touch",
    "href": "index.html#stay-in-touch",
    "title": "The Algorithm Codex",
    "section": "Stay in touch",
    "text": "Stay in touch\nMost of the chapters in this book are first published as standalone articles in The Computist Journal. Subscribing there is the best way to stay in the loop and get early access to most of the material.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_search.html",
    "href": "01_search.html",
    "title": "1  Basic Search",
    "section": "",
    "text": "1.1 Linear Search\nSearching is arguably the most important problem in Computer Science. In a very simplistic way, searching is at the core of critical applications like databases, and is the cornerstone of how the internet works.\nHowever, beyond this simple, superficial view of searching as an end in itself, you can also view search as means for general-purpose problem solving. When you are, for example, playing chess, what your brain is doing is, in a very fundamental way, searching for the optimal move–the only that most likely leads to winning.\nIn this sense, you can view almost all of Computer Science problems as search problems. In fact, a large part of this book will be devoted to search, in one way or another.\nIn this first chapter, we will look at the most explicit form of search: where we are explicitly given a set or collection of items, and asked to find one specific item.\nWe will start with the simplest, and most expensive kind of search, and progress towards increasingly more refined algorithms that exploit characteristics of the input items to minimize the time required to find the desired item, or determine if it’s not there at all.\nLet’s start by analyzing the simplest algorithm that does something non-trivial: linear search. Most of these algorithms work on the simplest data structure that we will see, the sequence.\nA sequence (Sequence class) is an abstract data type that represents a collection of items with no inherent structure, other than each element has an index.\nLinear search is the most basic form of search. We have a sequence of elements, and we must determine whether one specific element is among them. Since we cannot assume anything at all from the sequence, our only option is to check them all.\nOur first test will be a sanity check for simple cases:",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#linear-search",
    "href": "01_search.html#linear-search",
    "title": "1  Basic Search",
    "section": "",
    "text": "from typing import Sequence\n\ndef find[T](x:T, items: Sequence[T]) -&gt; bool:\n    for y in items:\n        if x == y:\n            return True\n\n    return False\n\nfrom codex.search.linear import find\n\ndef test_simple_list():\n    assert find(1, [1,2,3]) is True\n    assert find(2, [1,2,3]) is True\n    assert find(3, [1,2,3]) is True\n    assert find(4, [1,2,3]) is False",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#indexing-and-counting",
    "href": "01_search.html#indexing-and-counting",
    "title": "1  Basic Search",
    "section": "1.2 Indexing and Counting",
    "text": "1.2 Indexing and Counting\nThe find method is good to know if an element exists in a sequence, but it doesn’t tell us where. We can easily extend it to return an index. We thus define the index method, with the following condition: if index(x,l) == i then l[i] == x. That is, index returns the first index where we can find a given element x.\ndef index[T](x: T, items: Sequence[T]) -&gt; int | None:\n    for i,y in enumerate(items):\n        if x == y:\n            return i\n\n    return None\nWhen the item is not present in the sequence, we return None. We could raise an exception instead, but that would force a lot of defensive programming.\nLet’s write some tests!\nfrom codex.search.linear import index\n\ndef test_index():\n    assert index(1, [1,2,3]) == 0\n    assert index(2, [1,2,3]) == 1\n    assert index(3, [1,2,3]) == 2\n    assert index(4, [1,2,3]) is None\nAs a final step in the linear search paradigm, let’s consider the problem of finding not the first, but all occurrences of a given item. We’ll call this function count. It will return the number of occurrences of some item x in a sequence.\ndef count[T](x: T, items: Sequence[T]) -&gt; int:\n    c = 0\n\n    for y in items:\n        if x == y:\n            c += 1\n\n    return c\nLet’s write some simple tests for this method.\nfrom codex.search.linear import count\n\ndef test_index():\n    assert count(1, [1,2,3]) == 1\n    assert count(2, [1,2,2]) == 2\n    assert count(4, [1,2,3]) == 0",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#min-and-max",
    "href": "01_search.html#min-and-max",
    "title": "1  Basic Search",
    "section": "1.3 Min and Max",
    "text": "1.3 Min and Max\nLet’s now move to a slightly different problem. Instead of finding one specific element, we want to find the element that ranks minimum or maximum. Consider a sequence of numbers in an arbitrary order. We define the minimum (maximum) element as the element x such as x &lt;= y (x &gt;= y) for all y in the sequence.\nNow, instead of numbers, consider some arbitrary total ordering function f, such that f(x,y) &lt;= 0 if and only if x &lt;= y. This allows us to extend the notion of minimum and maximum to arbitrary data types.\nLet’s formalize this notion as a Python type alias. We will define an Ordering as a function that has this signature:\nfrom typing import Callable\n\ntype Ordering[T] = Callable[[T,T], int]\nNow, to make things simple for the simplest cases, let’s define a default ordering function that just delegates to the items own &lt;= implementation. This way we don’t have to reinvent the wheel with numbers, strings, and all other natively comparable items.\ndef default_order(x, y):\n    if x &lt; y:\n        return -1\n    elif x == y:\n        return 0\n    else:\n        return 1\nLet’s write the minimum method using this convention. Since we have no knowledge of the structure of the sequence other than it supports partial ordering, we have to test all possible items, like before. But now, instead of returning as soon as we find the “correcOf course, we t” item, we simply store the minimum item we’ve seen so far, and return at the end of the for loop. This guarantees we have seen all the items, and thus the minimum among them must be the one we have marked.\nfrom codex.types import Ordering, default_order\n\ndef minimum[T](items: Sequence[T], f: Ordering[T] = None) -&gt; T:\n    if f is None:\n        f = default_order\n\n    m = None\n\n    for x in items:\n        if m is None or f(x,m) &lt;= 0:\n            m = x\n\n    return m\nThe minimum method can fail only if the items sequence is empty. In the same manner, we can implement maximum. But instead of coding another method with the same functionality, which is not very DRY, we can leverage the fact that we are passing an ordering function that we can manipulate.\nConsider an arbitrary ordering function f such f(x,y) &lt;= 0. This means by definition that x &lt;= y. Now we want to define another function g such that g(y,x) &lt;= 0, that is, it inverts the result of f. We can do this very simply by swaping the inputs in f.\ndef maximum[T](items: Sequence[T], f: Ordering[T] = None) -&gt; T:\n    if f is None:\n        f = default_order\n\n    return minimum(items, lambda x,y: f(y,x))\nWe can easily code a couple of test methods for this new functionality.\nfrom codex.search.linear import minimum, maximum\n\ndef test_minmax():\n    items = [4,2,6,5,7,1,0]\n\n    assert minimum(items) == 0\n    assert maximum(items) == 7",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#conclusion",
    "href": "01_search.html#conclusion",
    "title": "1  Basic Search",
    "section": "1.4 Conclusion",
    "text": "1.4 Conclusion\nLinear search is a powerful paradigm precisely because it is universal. Whether we are checking for the existence of an item, finding its index, or identifying the minimum or maximum element in a collection, the exhaustive approach provides absolute certainty. No matter the nature of the data, if we test every single element and skim through every possibility, the problem will be solved.\nThe primary drawback of this certainty is the cost: some search spaces are simply too vast to be traversed one item at a time. To achieve better performance, we must move beyond the assumption of an unstructured sequence. We need to know more about the search space and impose some level of structure.\nIn the next chapter, we will explore the most straightforward structure we can impose: order. We will see how knowing the relative position of items allows us to implement what is arguably the most efficient and beautiful algorithm ever designed.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html",
    "href": "02_binary.html",
    "title": "2  Efficient Search",
    "section": "",
    "text": "2.1 Binary Search\nNow that we have started considering ordered sets, we can introduce what is arguably the most beautiful algorithm in the history of Computer Science: binary search. A quintessential algorithm that shows how a well-structured search space is exponentially easier to search than an arbitrary one.\nTo build some intuition for binary search, let’s consider we have an ordered sequence of items; that is, we always have that if i &lt; j, then l[i] &lt;= l[j]. This simple constraint introduces a very powerful condition in our search problem: if we are looking for x, and x &lt; y, then we know no item after y in the sequence can be x.\nConvince yourself of this simple truth before moving on.\nThis fundamentally changes how fast we can search. Why? Because now every test that we perform–every time we ask whether x &lt; y for some y–we gain a lot of information, not only about y, but about every other item greater or equal than y.\nThis is the magical leap we always need in order to write a fast algorithm–fast as in, it doesn’t need to check every single thing. We need a way to gather more information from every operation, so we have to do less operations. Let’s see how we can leverage this powerful intuition to make search not only faster, but exponentially faster when items are ordered.\nConsider the set of items \\(x_1, ..., y, ..., x_n\\). We are searching for item \\(x\\), and we choose to test whether \\(x \\leq y\\). We have two choices, either \\(x \\leq y\\), or, on the contrary, \\(x &gt; y\\). We want to gain the maximum amount of information in either case. The question is, how should we pick \\(y\\)?\nIf we pick \\(y\\) too close to either end, we can get lucky and cross off a large number of items. For example if \\(y\\) is in the last 5% of the sequence, and it turns out \\(x &gt; y\\), we have just removed the first 95% of the sequence without looking at it! But of course, we won’t get that lucky too often. In fact, if \\(x\\) is a random input, it could potentially be anywhere in the sequence. Under the fairly mild assumption that \\(x\\) should be uniformly distributed among all indices in the sequences, we will get this lucky exactly 5% of the time. The other 95! we have almost as much work to do as in the beginning.\nIt should be obvious by now that the best way to pick \\(y\\) either case is to choose the middle of the sequence. In that way I always cross off 50% of the items, regardless of luck. This is good, we just removed a huge chunk. But it gets even better.\nNow, instead of looking for \\(x\\) linearly in the remaining 50% of the items, we do the exact same thing again! We take the middle point of the current half, and now we can cross off another 25% of the items. If we keep repeating this over and over, how fast will we be left with just one item? Keep that thought in mind.\nBefore doing the math, here is the most straightforward implemenation of binary search. We will use two indices, l(eft) and r(ight) to keep track of the current sub-sequence we are analyzing. As long as l &lt;= r there is at least one item left to test. Once l &gt; r, we must conclude x is not in the sequence.\nHere goes the code.\nHere is a minimal test.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html#binary-search",
    "href": "02_binary.html#binary-search",
    "title": "2  Efficient Search",
    "section": "",
    "text": "from typing import Sequence\nfrom codex.types import Ordering, default_order\n\ndef binary_search[T](\n    x: T, items: Sequence[T], f: Ordering[T] = None\n) -&gt; int | None:\n    if f is None:\n        f = default_order\n\n    l, r = 0, len(items)-1\n\n    while l &lt;= r:\n        m = (l + r) // 2\n        res = f(x, items[m])\n\n        if res == 0:\n            return m\n        elif res &lt; 0:\n            r = m - 1\n        else:\n            l = m + 1\n\nfrom codex.search.binary import binary_search\n\ndef test_binary_search():\n    items = [0,1,2,3,4,5,6,7,8,9]\n\n    assert binary_search(3, items) == 3\n    assert binary_search(10, items) is None",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html#bisection",
    "href": "02_binary.html#bisection",
    "title": "2  Efficient Search",
    "section": "2.2 Bisection",
    "text": "2.2 Bisection\nStandard binary search is excellent for determining if an element exists, but it provides no guarantees about which index is returned if the sequence contains duplicates. In many applications—such as range queries or maintaining a sorted list—we need to find the specific boundaries where an element resides or where it should be inserted to maintain order.\nThis is the problem of bisection. We define two variants: bisect_left and bisect_right.\nThe bisect_left function finds the first index where an element x could be inserted while maintaining the sorted order of the sequence. If x is already present, the insertion point will be before (to the left of) any existing entries. Effectively, it returns the index of the first element that is not “less than” x.\ndef bisect_left[T](\n    x: T, items: Sequence[T], f: Ordering[T] = None\n) -&gt; int:\n    if f is None:\n        f = default_order\n\n    l, r = 0, len(items)\n\n    while l &lt; r:\n        m = (l + r) // 2\n        if f(items[m], x) &lt; 0:\n            l = m + 1\n        else:\n            r = m\n\n    return l\nThe logic here is subtle: instead of returning immediately when an element matches, we keep narrowing the window until l and r meet. By setting r = m when items[m] &gt;= x, we ensure the right boundary eventually settles on the first occurrence.\nConversely, bisect_right (sometimes called bisect_upper) finds the last possible insertion point. If x is present, the index returned will be after (to the right of) all existing entries. This is useful for finding the index of the first element that is strictly “greater than” x.\ndef bisect_right[T](\n    x: T, items: Sequence[T], f: Ordering[T] = None\n) -&gt; int:\n    if f is None:\n        f = default_order\n\n    l, r = 0, len(items)\n\n    while l &lt; r:\n        m = (l + r) // 2\n        if f(x, items[m]) &lt; 0:\n            r = m\n        else:\n            l = m + 1\n\n    return l\nIn this variant, we only move the left boundary l forward if x &gt;= items[m], which pushes the search toward the end of a block of identical values.\nSince both functions follow the same halving principle as standard binary search, their performance characteristics are identical:\n\nTime Complexity: \\(O(\\log n)\\), as we halve the search space in every iteration of the while loop.\nSpace Complexity: \\(O(1)\\), as we only maintain two integer indices regardless of the input size.\n\nTo ensure these boundaries are calculated correctly, especially with duplicate elements, we use the following test cases:\nfrom codex.search.binary import bisect_left, bisect_right\n\ndef test_bisection_boundaries():\n    # Sequence with a \"block\" of 2s\n    items = [1, 2, 2, 2, 3]\n\n    # First index where 2 is (or could be)\n    assert bisect_left(2, items) == 1\n\n    # Index after the last 2\n    assert bisect_right(2, items) == 4\n\n    # If element is missing, both return the same insertion point\n    assert bisect_left(1.5, items) == 1\n    assert bisect_right(1.5, items) == 1\n\ndef test_bisection_extremes():\n    items = [1, 2, 3]\n    assert bisect_left(0, items) == 0\n    assert bisect_right(4, items) == 3",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html#conclusion",
    "href": "02_binary.html#conclusion",
    "title": "2  Efficient Search",
    "section": "2.3 Conclusion",
    "text": "2.3 Conclusion\nSearching is arguably the most important problem in Computer Science. In this first chapter, we have only scratched the surface of this vast field, but in doing so, we have discovered one of the fundamental truths of computation: structure matters–a lot.\nWhen we know nothing about the structure of our problem or the collection of items we are searching through, we have no choice but to rely on exhaustive methods like linear search. In these cases, we must check every single item to determine if it is the one we care about.\nHowever, as soon as we introduce some structure–specifically, some order–the landscape changes completely. Binary search allows us to exploit this structure to find an element as fast as is theoretically possible, reducing our workload from a linear progression to a logarithmic one.\nThis realization that we can trade a bit of organizational effort for a massive gain in search efficiency is the perfect segue for our next chapter. If searching is easier when items are ordered, then we must understand the process of establishing that order. We must talk about sorting.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "03_sort.html",
    "href": "03_sort.html",
    "title": "3  Basic Sorting",
    "section": "",
    "text": "3.1 Selection Sort\nAs we discovered in the previous chapter, structure is the secret ingredient that makes computation efficient. Searching through an unordered collection is a tedious, linear process, but searching through an ordered one is exponentially faster.\nSorting is the process of establishing this order. Formally, we want to take a sequence of items and rearrange them into a new sequence where, for any two indices \\(i\\) and \\(j\\), if \\(i &lt; j\\), then \\(x_i &lt; x_j\\). In this chapter, we explore the most fundamental ways to achieve this, building our intuition from simple observation to a deeper structural analysis of the sorting problem itself.\nThe most intuitive way to sort a list is to think about the destination. If we are building a sorted sequence, the very first element (index 0) must be the minimum element of the entire collection. Once that is in place, the second element (index 1) must be the minimum of everything that remains, and so on.\nIn Selection Sort, we stand at each position in the array and ask: “Which element belongs here?” To answer, we scan the unsorted portion of the list, find the minimum, and swap it into our current position.\nWe implement this by directly searching for the minimum index in each iteration rather than calling a helper function, keeping the logic self-contained. Because we must scan the remaining items for every single position in the list, we perform roughly \\(1 + 2 + 3 + ... (n-1) = n(n-1)/2\\) comparisons, leading to a time complexity of \\(O(n^2)\\).",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#selection-sort",
    "href": "03_sort.html#selection-sort",
    "title": "3  Basic Sorting",
    "section": "",
    "text": "from typing import MutableSequence\nfrom codex.types import Ordering, default_order\n\ndef selection_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    n = len(items)\n    for i in range(n):\n        # Find the index of the minimum element in the unsorted suffix\n        min_idx = i\n        for j in range(i + 1, n):\n            if f(items[j], items[min_idx]) &lt; 0:\n                min_idx = j\n\n        # Swap it into the current position\n        items[i], items[min_idx] = items[min_idx], items[i]",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#insertion-sort",
    "href": "03_sort.html#insertion-sort",
    "title": "3  Basic Sorting",
    "section": "3.2 Insertion Sort",
    "text": "3.2 Insertion Sort\nWe can flip the narrative of Selection Sort. Instead of standing at a position and looking for the right element, we can take an element and look for its right position. This is how most people sort a hand of cards.\nWe assume that everything behind our current position is already sorted. We take the next element and ask: “How far back must I move this element so that the sequence remains sorted?” We shift it backward, swapping it with its predecessor, until it finds its rightful place.\ndef insertion_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    for i in range(1, len(items)):\n        j = i\n        # Move the element backward as long as it is smaller than its predecessor\n        while j &gt; 0 and f(items[j], items[j-1]) &lt; 0:\n            items[j], items[j-1] = items[j-1], items[j]\n            j -= 1\nThe first element is sorted by definition. The second element either stays put or moves before the first. The third moves until it is in the correct spot relative to the first two. In the worst case (a reverse-sorted list), this also results again in \\(O(n^2)\\) operations, but it is remarkably efficient for lists that are already “nearly sorted.”",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#the-geometry-of-inversions",
    "href": "03_sort.html#the-geometry-of-inversions",
    "title": "3  Basic Sorting",
    "section": "3.3 The Geometry of Inversions",
    "text": "3.3 The Geometry of Inversions\nTo understand why these algorithms are all quadratic in complexity, we need to look at the structure of “unsortedness.” Whenever a list in unsorted, is because we can find at least a couple of elements that are out of place. This means some \\(x_i &gt; x_j\\) where \\(i &lt; j\\). We define an inversion as any pair of such items. A sorted list has zero inversions.\nWith this idea in place, we can see sorting as “just” the problem of reducing the number of inversions down to zero. Any algorithm that does progress towards reducing the number of inversions is actually sorting. And a crucial insight is that there can be at most \\(O(n^2)\\) inversions in any sequence of size \\(n\\).\nSelection sort reduces up to \\(n\\) inversions with each swap, that is, all inversions relative to the current minimum element. But every swap requires up to \\(n\\) comparisons, so we get \\(O(n^2)\\) steps. Insertion sort reduces at most one inversion each step, by moving one item forward, thus it will require \\(O(n^2)\\) steps to eliminate that many inversions.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#bubble-sort",
    "href": "03_sort.html#bubble-sort",
    "title": "3  Basic Sorting",
    "section": "3.4 Bubble Sort",
    "text": "3.4 Bubble Sort\nLet’s now build an algorithm based on this idea of eliminating inversions directly. A first guess could be, lets try to find a pair of inverted items and swap them. But we must be careful, if we do indiscrimantely, we might end up fixing one inversion but creating other inversions.\nHowever, a powerful idea that we won’t formally proof is that if a list has any inversions, there must be at least one inversion between two consecutive elements. If we fix these local inversions, we eventually fix them all. And fixing an inversion between consecutive elements cannot create new inversions. This is the heart of Bubble Sort: we repeatedly step through the list and swap adjacent items that are out of order.\ndef bubble_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    n = len(items)\n    for i in range(n):\n        swapped = False\n        for j in range(0, n - i - 1):\n            # If we find a consecutive inversion, fix it\n            if f(items[j+1], items[j]) &lt; 0:\n                items[j], items[j+1] = items[j+1], items[j]\n                swapped = True\n\n        # If no swaps occurred, the list is already sorted\n        if not swapped:\n            break",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#verification",
    "href": "03_sort.html#verification",
    "title": "3  Basic Sorting",
    "section": "3.5 Verification",
    "text": "3.5 Verification\nTo ensure these three fundamental sorting approaches work as intended, we can run them against a set of standard cases.\nimport pytest\nfrom codex.sort.basic import selection_sort, insertion_sort, bubble_sort\n\n@pytest.mark.parametrize(\"sort_fn\", [selection_sort, insertion_sort, bubble_sort])\ndef test_sorting_algorithms(sort_fn):\n    items = [4, 2, 7, 1, 3]\n    sort_fn(items)\n    assert items == [1, 2, 3, 4, 7]\n\n    # Already sorted\n    items = [1, 2, 3]\n    sort_fn(items)\n    assert items == [1, 2, 3]\n\n    # Reverse sorted\n    items = [3, 2, 1]\n    sort_fn(items)\n    assert items == [1, 2, 3]",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#conclusion",
    "href": "03_sort.html#conclusion",
    "title": "3  Basic Sorting",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nSelection, Insertion, and Bubble sort are all \\(O(n^2)\\) algorithms. The reason is structural: in the worst case, a list of size can have \\(O(n^2)\\) inversions. Since each swap in these algorithms only fixes one inversion at a time–in the best case–we are forced to perform a quadratic number of operations.\nTo break this ceiling and reach the theoretical limit of \\(O(n^2)\\), we need to be more clever. We need algorithms that can fix many inversions with a single operation. This “divide and conquer” approach will be the focus of our next chapter: Efficient Sorting.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  }
]