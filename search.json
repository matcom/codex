[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Algorithm Codex",
    "section": "",
    "text": "Preface\nWelcome to The Algorithm Codex.\nThis book is a repository of common algorithms used in all areas of computer science. It contains reference implementations in Python for many well-known (and some not-so-much) algorithms spanning from simple linear search to sorting, graphs, computational geometry, data structures, flow networks, game theory, number theory, optimization, and many other fields.\nWe wrote this book to serve as a complement for the main bibliography in a typical Computer Science major. You will not find comprehensive theory of algorithms in this book, or detailed analyses. However, we do present some basic intuitions into why most of the presented algorithms work and a back-of-the-envelope cost analysis whenever it makes sense.\nThe order in which algorithms are presented is our best attempt to build the most complex ideas on top of the simpler ones. We start with basic algorithms that everyone can understand and progressively move towards the more advanced.\nThe algorithms are presented in a literate programming format. The gist is that we combine code and prose in the best possible way to maximize understanding. Actually, the source code is generated from the book source, and not the other way around–that is what literate programming is, after all. Accompanying this book, you will find an open source repository with the exact implementations in this book.\nYou can read the book online at https://matcom.github.io/codex/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-is-this-book-about",
    "href": "index.html#what-is-this-book-about",
    "title": "The Algorithm Codex",
    "section": "What is this book about?",
    "text": "What is this book about?\nThe Algorithm Codex is designed as a multifaceted resource for a broad spectrum of the computational community. Students will find it to be a vital bridge between theoretical university lectures and the concrete reality of modern implementations, serving as a complement to standard academic curricula.\nFor professors, the book provides a library of clean, pedagogical code that emphasizes structural clarity over complexity. Researchers can utilize these implementations as a reliable reference for standard algorithmic logic when prototyping new ideas, while developers will gain a deeper understanding of the underlying abstractions that power their daily tools.\nUltimately, even enthusiasts driven by curiosity will find the logical elegance of these algorithms accessible through the lens of literate programming.\nIt is equally important to clarify what this project represents by stating that this is not a programming book; we assume that the reader is already proficient in Python 3 and understands the fundamental principles of software development. We deliberately avoid pseudo-code in favor of actual, runnable Python to ensure that no logic is lost in translation.\nFurthermore, this work is not an algorithm design book and should not be viewed as a replacement for comprehensive theoretical texts such as Introduction to Algorithms. We do not offer formal proofs of correctness or exhaustive mathematical analysis, choosing instead to build intuition through back-of-the-envelope cost estimates and prose.\nFinally, the Codex is not a production cookbook meant for direct copy-pasting into high-stakes environments; our code is optimized for readability and educational insight rather than the micro-optimizations required for production-level software.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#content-of-the-book",
    "href": "index.html#content-of-the-book",
    "title": "The Algorithm Codex",
    "section": "Content of the Book",
    "text": "Content of the Book\nThe Algorithm Codex is organized into several major parts, designed to take you from foundational concepts to specialized domains and the limits of computation:\n\nSearching and Sorting: We establish the core intuitions of algorithmic efficiency by exploring how to find and organize data in linear and logarithmic time.\nFundamental Data Structures: We implement essential abstractions—including linked lists, stacks, queues, and hash tables—that serve as the building blocks for more complex systems.\nTrees: This part covers hierarchical data, from binary search trees to self-balancing structures and specialized variants like heaps and tries.\nString Algorithms: We focus on pattern matching and text processing, covering algorithms from exact matching (KMP, Boyer-Moore) to advanced suffix structures.\nGraphs: A significant section dedicated to relational data, covering traversals, shortest paths, spanning trees, and flow networks.\nDynamic Programming and Greedy Algorithms: We delve into powerful paradigms for solving optimization problems by exploiting subproblem structure and local optimality.\nSpecialized Domains: We explore deep subregions of Computer Science, including computational geometry, number theory, and game theory.\nAdvanced Complexity: The book concludes with the frontiers of computation, exploring NP-completeness, approximation algorithms, and randomized approaches.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-the-coding-style",
    "href": "index.html#about-the-coding-style",
    "title": "The Algorithm Codex",
    "section": "About the coding style",
    "text": "About the coding style\nThe code in this book is written in Python 3, specifically the 3.13 version.\nWe make extensive use of Python’s generic syntax to write clean but fully typed methods that leverage the best and most modern practices in software development. Other than that, the code is often written in the simplest possible way that works. We don’t make unnecessary optimizations like taking bounds out of a loop. On the other hand, our code is optimized in the algorithmic sense; it is fast because it exploits the inherent structure of the problem.\nSince most of our code is pure, functional algorithms, we often rely on public, plain Python functions. We thus have very few classes, and the ones we have are very simple, often nothing but data stores. However, we do make heavy use of protocols and abstract classes, especially those in the Python standard library like sequences, maps, and queues.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#support-the-algorithm-codex",
    "href": "index.html#support-the-algorithm-codex",
    "title": "The Algorithm Codex",
    "section": "Support The Algorithm Codex",
    "text": "Support The Algorithm Codex\nThis book is free, as in free beer and free speech, and it will always be.\nThe book content is licensed CC-BY-NC-SA, that means you are free to share the book in any format (HTML, ePUB, PDF) with anyone, and produce any derivatives you want, as long as you also share those freely for posterity.\nThe source code is licensed MIT, and thus you can use the algorithms implemented here for anything, including classes, academic work, but also writing commercial software.\nThe only thing you cannot do is resell the book itself or any derivative work like lecture notes, translations, etc.\nIf you want to support this effort, the best way to do is to buy the official PDF.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#stay-in-touch",
    "href": "index.html#stay-in-touch",
    "title": "The Algorithm Codex",
    "section": "Stay in touch",
    "text": "Stay in touch\nMost of the chapters in this book are first published as standalone articles in The Computist Journal. Subscribing there is the best way to stay in the loop and get early access to most of the material.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00_intro.html",
    "href": "00_intro.html",
    "title": "1  Foundations",
    "section": "",
    "text": "1.1 What is an Algorithm?\nBefore we begin our journey through specific algorithms, we must establish the ground on which we stand. To study algorithms is to study the limits of what can be computed and the cost of doing so.\nAt its simplest, an algorithm is a procedure that takes an input and produces an output. However, in this Codex, we view an algorithm as a formal mathematical object—a precise strategy that exploits the structure of data to achieve an outcome efficiently.\nTo be considered a valid algorithm in our context, a procedure must satisfy several key characteristics:\nMost academic texts rely on pseudo-code—a high-level, informal description of an algorithm. While pseudo-code is useful for broad strokes, it often hides subtle complexities and can be interpreted in multiple ways.\nIn The Algorithm Codex, we deliberately avoid pseudo-code in favor of actual, runnable Python 3.13. By using a real programming language, we ensure that every operation is precisely defined and that the implementations you see are ready to be tested, scrutinized, and executed. This approach removes the “translation layer” between theory and practice, making the logic transparent and absolute.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "00_intro.html#what-is-an-algorithm",
    "href": "00_intro.html#what-is-an-algorithm",
    "title": "1  Foundations",
    "section": "",
    "text": "Finiteness: The description of the algorithm itself must be finite. Furthermore, for any valid input, the algorithm must always finish within a finite amount of time.\nCorrectness: The algorithm must always produce the correct answer for every valid input within its problem class.\nDefiniteness (Formality): An algorithm is a formal procedure. It must be described in a language that admits no ambiguity regarding the operations to be performed. Historically, this has been achieved through mathematical notation; in this book, we use the Python programming language.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "00_intro.html#analyzing-algorithms",
    "href": "00_intro.html#analyzing-algorithms",
    "title": "1  Foundations",
    "section": "1.2 Analyzing Algorithms",
    "text": "1.2 Analyzing Algorithms\nFollowing the definition of what an algorithm is, we must establish a framework for evaluating them. Once an implementation is complete, the work of a computer scientist is only beginning. We must subject our solution to a rigorous three-step analysis to ensure it is not just a working program, but a complete solution to a computational problem.\nWhen we finish writing an algorithm, we must ask ourselves three fundamental questions. Only when all three are answered can we consider our work in computer science truly satisfied.\nIs it correct?\nThe first and most critical question is whether the algorithm always produces the expected output for any valid input. This is the property of correctness. While this book avoids exhaustive formal proofs, we will always strive to provide a deep, intuitive explanation of why the logic holds. We focus on the underlying mechanics of the algorithm and how it handles corner cases—those extreme or unusual inputs where many naive solutions fail.\nHow efficient is it?\nOnce we are certain the algorithm is correct, we must quantify its cost. We ask: How efficient is this in terms of time and space?. Using the scaling intuition of Big O notation, we analyze how the algorithm’s resource requirements grow as the input size n increases . We look for the “bottlenecks” in the logic and determine whether the primary cost comes from the number of operations performed or the amount of memory consumed.\nIs it optimal?\nThe final question is perhaps the most profound: Is this the most efficient algorithm possible, or can there be a better one? We are not just looking for a “fast” algorithm; we are looking for the theoretical limits of the problem itself. Throughout this Codex, we will try to provide intuitive proofs of optimality—explaining why a certain complexity (like O(nlogn) for comparison-based sorting) cannot be improved upon.\nWhen we can prove—even intuitively—that we have reached the optimal efficiency for a correct algorithm, we have solved the problem completely. At that point, the computational task is no longer a mystery; it is a solved piece of the science of computation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "00_intro.html#measuring-efficiency-time-space-and-scaling",
    "href": "00_intro.html#measuring-efficiency-time-space-and-scaling",
    "title": "1  Foundations",
    "section": "1.3 Measuring Efficiency: Time, Space, and Scaling",
    "text": "1.3 Measuring Efficiency: Time, Space, and Scaling\nOnce we have established that an algorithm is correct, we must ask how much it “costs” to run. In this book, we care about two primary resources: Time (the number of operations performed) and Space (the amount of memory required).\n\n1.3.1 The Unitary Cost Model\nBecause hardware changes so rapidly, it is rarely useful to talk about runtime in terms of seconds or memory in terms of megabytes. A sorting algorithm that takes ten seconds on a vintage machine might take a fraction of a millisecond on a modern supercomputer. To remain hardware-agnostic, we fix a unitary unit of cost.\nWe assume that “atomic” operations—such as variable assignment, basic arithmetic, or printing to the console—each take exactly one unit of time. Similarly, we assume that atomic data types—like a single number or a character—occupy one unit of memory. By counting these units, we can discuss the cost of an algorithm as a mathematical function of its input.\nHowever, we rarely care about the absolute number of steps. Knowing that a specific sort takes exactly 1,024 operations is less useful than knowing how that cost grows as the input size \\(n\\) increases.\nThe core of algorithmic analysis is scaling. For example:\n\nConstant Cost (\\(O(1)\\)): If you have a list of items and you want to access the 42nd element, that operation has a unitary cost of 1. It does not matter if the list has 1,000 items, 1 million items, or 1 billion items; the effort required to jump to that specific index remains the same.\nLinear Cost (\\(O(n)\\)): If you want to count every item in that list, you must visit each one. If the size of the input doubles, the cost of the operation doubles. If you have 1,000 items, the cost is 1,000; if you have 1 million, the cost is 1 million.\n\nTo formalize these scaling patterns, we use asymptotic notation, a terminology borrowed from mathematical analysis. This allows us to categorize algorithms into growth classes:\n\n\\(O(1)\\) - Constant Time: The cost is independent of the input size.\n\\(O(n)\\) - Linear Time: The cost grows in direct proportion to the input size.\n\\(O(n^2)\\) - Quadratic Time: The cost grows with the square of the input size, often seen in algorithms with nested loops.\n\nBy focusing on these growth rates, we can determine the “efficiency ceiling” of our solutions and decide whether we have found the optimal approach for a given problem.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "00_intro.html#final-words",
    "href": "00_intro.html#final-words",
    "title": "1  Foundations",
    "section": "1.4 Final Words",
    "text": "1.4 Final Words\nNow that we have settled our expectations, you are ready to start the journey. It will be fast-paced but–I hope–really exciting. We will discover many algorithms, close to a hundred of them! And in each case, we ill ask ourselves these same three questions. And, surprisingly often, we will be able to answer them pretty well!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations</span>"
    ]
  },
  {
    "objectID": "part1.html",
    "href": "part1.html",
    "title": "Searching and Sorting",
    "section": "",
    "text": "What We Will Explore\nThis first part of The Algorithm Codex serves as our entry point into the science of computation. We begin with the most fundamental of tasks: finding and organizing data. While these problems may seem elementary, they reveal the deepest truth of computer science: structure is the primary driver of efficiency.\nIn this part, we transition from the exhaustive “brute force” methods of linear search to the elegant, logarithmic precision of binary search, and from the quadratic complexity of basic sorting to the theoretical limits of divide-and-conquer algorithms.\nThrough these chapters, we move from simple observations to a rigorous structural analysis of search spaces and the “geometry of inversions”:",
    "crumbs": [
      "Searching and Sorting"
    ]
  },
  {
    "objectID": "part1.html#what-we-will-explore",
    "href": "part1.html#what-we-will-explore",
    "title": "Searching and Sorting",
    "section": "",
    "text": "Basic Search: We start with the universal but expensive paradigm of linear search, establishing the baseline for what happens when we know nothing about our data.\nEfficient Search: We introduce binary search and bisection, demonstrating how an ordered search space allows us to gain the maximum possible information from every comparison.\nFundamental Sorting: We analyze Selection, Insertion, and Bubble sort, learning why is the natural ceiling for algorithms that fix only one or two inversions at a time.\nEfficient Sorting: We break the quadratic barrier using Merge Sort and Quick Sort, exploring the power of recursion to fix multiple inversions simultaneously through divide-and-conquer strategies.\nOrder Statistics: We solve the problem of selection—finding the -th smallest item—by leveraging partitioning logic to achieve linear time performance.\nLinear Time Sorting: We demonstrate that the limit can be bypassed entirely if we exploit domain-specific constraints, such as the discrete nature of integers, through Counting and Radix sort.",
    "crumbs": [
      "Searching and Sorting"
    ]
  },
  {
    "objectID": "part1.html#what-you-will-learn",
    "href": "part1.html#what-you-will-learn",
    "title": "Searching and Sorting",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nBy following this progression, you will develop the “algorithmic intuition” required to analyze and solve increasingly complex problems:\n\nThe Information Gain Principle: Why halving the search space leads to exponential efficiency gains.\nDivide and Conquer: How to break a monolithic problem into independent sub-problems that are easier to solve and combine.\nThe Geometry of Inversions: Understanding “unsortedness” as a structural property that can be measured and methodically reduced.\nRandomization as a Strategy: How to use probabilistic approaches to avoid pathological cases and ensure robust average-case performance.\nThe Power of Constraints: Why knowing the range or type of your input allows for optimizations that are mathematically impossible in a generic context.\n\nSearching and sorting are not just utility functions; they are the playground where we learn the rules of algorithmic negotiation. We are learning how much effort we must expend to impose order, and how much that order pays us back in search speed.",
    "crumbs": [
      "Searching and Sorting"
    ]
  },
  {
    "objectID": "01_search.html",
    "href": "01_search.html",
    "title": "2  Basic Search",
    "section": "",
    "text": "2.1 Linear Search\nSearching is arguably the most important problem in Computer Science. In a very simplistic way, searching is at the core of critical applications like databases, and is the cornerstone of how the internet works.\nHowever, beyond this simple, superficial view of searching as an end in itself, you can also view search as means for general-purpose problem solving. When you are, for example, playing chess, what your brain is doing is, in a very fundamental way, searching for the optimal move–the only that most likely leads to winning.\nIn this sense, you can view almost all of Computer Science problems as search problems. In fact, a large part of this book will be devoted to search, in one way or another.\nIn this first chapter, we will look at the most explicit form of search: where we are explicitly given a set or collection of items, and asked to find one specific item.\nWe will start with the simplest, and most expensive kind of search, and progress towards increasingly more refined algorithms that exploit characteristics of the input items to minimize the time required to find the desired item, or determine if it’s not there at all.\nLet’s start by analyzing the simplest algorithm that does something non-trivial: linear search. Most of these algorithms work on the simplest data structure that we will see, the sequence.\nA sequence (Sequence class) is an abstract data type that represents a collection of items with no inherent structure, other than each element has an index.\nLinear search is the most basic form of search. We have a sequence of elements, and we must determine whether one specific element is among them. Since we cannot assume anything at all from the sequence, our only option is to check them all.\nOur first test will be a sanity check for simple cases:",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#linear-search",
    "href": "01_search.html#linear-search",
    "title": "2  Basic Search",
    "section": "",
    "text": "from typing import Sequence\n\ndef find[T](x:T, items: Sequence[T]) -&gt; bool:\n    for y in items:\n        if x == y:\n            return True\n\n    return False\n\nfrom codex.search.linear import find\n\ndef test_simple_list():\n    assert find(1, [1,2,3]) is True\n    assert find(2, [1,2,3]) is True\n    assert find(3, [1,2,3]) is True\n    assert find(4, [1,2,3]) is False",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#indexing-and-counting",
    "href": "01_search.html#indexing-and-counting",
    "title": "2  Basic Search",
    "section": "2.2 Indexing and Counting",
    "text": "2.2 Indexing and Counting\nThe find method is good to know if an element exists in a sequence, but it doesn’t tell us where. We can easily extend it to return an index. We thus define the index method, with the following condition: if index(x,l) == i then l[i] == x. That is, index returns the first index where we can find a given element x.\ndef index[T](x: T, items: Sequence[T]) -&gt; int | None:\n    for i,y in enumerate(items):\n        if x == y:\n            return i\n\n    return None\nWhen the item is not present in the sequence, we return None. We could raise an exception instead, but that would force a lot of defensive programming.\nLet’s write some tests!\nfrom codex.search.linear import index\n\ndef test_index():\n    assert index(1, [1,2,3]) == 0\n    assert index(2, [1,2,3]) == 1\n    assert index(3, [1,2,3]) == 2\n    assert index(4, [1,2,3]) is None\nAs a final step in the linear search paradigm, let’s consider the problem of finding not the first, but all occurrences of a given item. We’ll call this function count. It will return the number of occurrences of some item x in a sequence.\ndef count[T](x: T, items: Sequence[T]) -&gt; int:\n    c = 0\n\n    for y in items:\n        if x == y:\n            c += 1\n\n    return c\nLet’s write some simple tests for this method.\nfrom codex.search.linear import count\n\ndef test_index():\n    assert count(1, [1,2,3]) == 1\n    assert count(2, [1,2,2]) == 2\n    assert count(4, [1,2,3]) == 0",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#min-and-max",
    "href": "01_search.html#min-and-max",
    "title": "2  Basic Search",
    "section": "2.3 Min and Max",
    "text": "2.3 Min and Max\nLet’s now move to a slightly different problem. Instead of finding one specific element, we want to find the element that ranks minimum or maximum. Consider a sequence of numbers in an arbitrary order. We define the minimum (maximum) element as the element x such as x &lt;= y (x &gt;= y) for all y in the sequence.\nNow, instead of numbers, consider some arbitrary total ordering function f, such that f(x,y) &lt;= 0 if and only if x &lt;= y. This allows us to extend the notion of minimum and maximum to arbitrary data types.\nLet’s formalize this notion as a Python type alias. We will define an Ordering as a function that has this signature:\nfrom typing import Callable\n\ntype Ordering[T] = Callable[[T,T], int]\nNow, to make things simple for the simplest cases, let’s define a default ordering function that just delegates to the items own &lt;= implementation. This way we don’t have to reinvent the wheel with numbers, strings, and all other natively comparable items.\ndef default_order(x, y):\n    if x &lt; y:\n        return -1\n    elif x == y:\n        return 0\n    else:\n        return 1\nLet’s write the minimum method using this convention. Since we have no knowledge of the structure of the sequence other than it supports partial ordering, we have to test all possible items, like before. But now, instead of returning as soon as we find the correct item, we simply store the minimum item we’ve seen so far, and return at the end of the for loop. This guarantees we have seen all the items, and thus the minimum among them must be the one we have marked.\nfrom codex.types import Ordering, default_order\n\ndef minimum[T](items: Sequence[T], f: Ordering[T] = None) -&gt; T:\n    if f is None:\n        f = default_order\n\n    m = None\n\n    for x in items:\n        if m is None or f(x,m) &lt;= 0:\n            m = x\n\n    return m\nThe minimum method can fail only if the items sequence is empty. In the same manner, we can implement maximum. But instead of coding another method with the same functionality, which is not very DRY, we can leverage the fact that we are passing an ordering function that we can manipulate.\nConsider an arbitrary ordering function f such f(x,y) &lt;= 0. This means by definition that x &lt;= y. Now we want to define another function g such that g(y,x) &lt;= 0, that is, it inverts the result of f. We can do this very simply by swaping the inputs in f.\ndef maximum[T](items: Sequence[T], f: Ordering[T] = None) -&gt; T:\n    if f is None:\n        f = default_order\n\n    return minimum(items, lambda x,y: f(y,x))\nWe can easily code a couple of test methods for this new functionality.\nfrom codex.search.linear import minimum, maximum\n\ndef test_minmax():\n    items = [4,2,6,5,7,1,0]\n\n    assert minimum(items) == 0\n    assert maximum(items) == 7",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "01_search.html#conclusion",
    "href": "01_search.html#conclusion",
    "title": "2  Basic Search",
    "section": "2.4 Conclusion",
    "text": "2.4 Conclusion\nLinear search is a powerful paradigm precisely because it is universal. Whether we are checking for the existence of an item, finding its index, or identifying the minimum or maximum element in a collection, the exhaustive approach provides absolute certainty. No matter the nature of the data, if we test every single element and skim through every possibility, the problem will be solved.\nThe primary drawback of this certainty is the cost: some search spaces are simply too vast to be traversed one item at a time. To achieve better performance, we must move beyond the assumption of an unstructured sequence. We need to know more about the search space and impose some level of structure.\nIn the next chapter, we will explore the most straightforward structure we can impose: order. We will see how knowing the relative position of items allows us to implement what is arguably the most efficient and beautiful algorithm ever designed.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html",
    "href": "02_binary.html",
    "title": "3  Efficient Search",
    "section": "",
    "text": "3.1 Binary Search\nNow that we have started considering ordered sets, we can introduce what is arguably the most beautiful algorithm in the history of Computer Science: binary search. A quintessential algorithm that shows how a well-structured search space is exponentially easier to search than an arbitrary one.\nTo build some intuition for binary search, let’s consider we have an ordered sequence of items; that is, we always have that if i &lt; j, then l[i] &lt;= l[j]. This simple constraint introduces a very powerful condition in our search problem: if we are looking for x, and x &lt; y, then we know no item after y in the sequence can be x.\nConvince yourself of this simple truth before moving on.\nThis fundamentally changes how fast we can search. Why? Because now every test that we perform–every time we ask whether x &lt; y for some y–we gain a lot of information, not only about y, but about every other item greater or equal than y.\nThis is the magical leap we always need in order to write a fast algorithm–fast as in, it doesn’t need to check every single thing. We need a way to gather more information from every operation, so we have to do less operations. Let’s see how we can leverage this powerful intuition to make search not only faster, but exponentially faster when items are ordered.\nConsider the set of items \\(x_1, ..., y, ..., x_n\\). We are searching for item \\(x\\), and we choose to test whether \\(x \\leq y\\). We have two choices, either \\(x \\leq y\\), or, on the contrary, \\(x &gt; y\\). We want to gain the maximum amount of information in either case. The question is, how should we pick \\(y\\)?\nIf we pick \\(y\\) too close to either end, we can get lucky and cross off a large number of items. For example if \\(y\\) is in the last 5% of the sequence, and it turns out \\(x &gt; y\\), we have just removed the first 95% of the sequence without looking at it! But of course, we won’t get that lucky too often. In fact, if \\(x\\) is a random input, it could potentially be anywhere in the sequence. Under the fairly mild assumption that \\(x\\) should be uniformly distributed among all indices in the sequences, we will get this lucky exactly 5% of the time. The other 95! we have almost as much work to do as in the beginning.\nIt should be obvious by now that the best way to pick \\(y\\) either case is to choose the middle of the sequence. In that way I always cross off 50% of the items, regardless of luck. This is good, we just removed a huge chunk. But it gets even better.\nNow, instead of looking for \\(x\\) linearly in the remaining 50% of the items, we do the exact same thing again! We take the middle point of the current half, and now we can cross off another 25% of the items. If we keep repeating this over and over, how fast will we be left with just one item? Keep that thought in mind.\nBefore doing the math, here is the most straightforward implemenation of binary search. We will use two indices, l(eft) and r(ight) to keep track of the current sub-sequence we are analyzing. As long as l &lt;= r there is at least one item left to test. Once l &gt; r, we must conclude x is not in the sequence.\nHere goes the code.\nHere is a minimal test.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html#binary-search",
    "href": "02_binary.html#binary-search",
    "title": "3  Efficient Search",
    "section": "",
    "text": "from typing import Sequence\nfrom codex.types import Ordering, default_order\n\ndef binary_search[T](\n    x: T, items: Sequence[T], f: Ordering[T] = None\n) -&gt; int | None:\n    if f is None:\n        f = default_order\n\n    l, r = 0, len(items)-1\n\n    while l &lt;= r:\n        m = (l + r) // 2\n        res = f(x, items[m])\n\n        if res == 0:\n            return m\n        elif res &lt; 0:\n            r = m - 1\n        else:\n            l = m + 1\n\nfrom codex.search.binary import binary_search\n\ndef test_binary_search():\n    items = [0,1,2,3,4,5,6,7,8,9]\n\n    assert binary_search(3, items) == 3\n    assert binary_search(10, items) is None",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html#bisection",
    "href": "02_binary.html#bisection",
    "title": "3  Efficient Search",
    "section": "3.2 Bisection",
    "text": "3.2 Bisection\nStandard binary search is excellent for determining if an element exists, but it provides no guarantees about which index is returned if the sequence contains duplicates. In many applications—such as range queries or maintaining a sorted list—we need to find the specific boundaries where an element resides or where it should be inserted to maintain order.\nThis is the problem of bisection. We define two variants: bisect_left and bisect_right.\nThe bisect_left function finds the first index where an element x could be inserted while maintaining the sorted order of the sequence. If x is already present, the insertion point will be before (to the left of) any existing entries. Effectively, it returns the index of the first element that is not “less than” x.\ndef bisect_left[T](\n    x: T, items: Sequence[T], f: Ordering[T] = None\n) -&gt; int:\n    if f is None:\n        f = default_order\n\n    l, r = 0, len(items)\n\n    while l &lt; r:\n        m = (l + r) // 2\n        if f(items[m], x) &lt; 0:\n            l = m + 1\n        else:\n            r = m\n\n    return l\nThe logic here is subtle: instead of returning immediately when an element matches, we keep narrowing the window until l and r meet. By setting r = m when items[m] &gt;= x, we ensure the right boundary eventually settles on the first occurrence.\nConversely, bisect_right (sometimes called bisect_upper) finds the last possible insertion point. If x is present, the index returned will be after (to the right of) all existing entries. This is useful for finding the index of the first element that is strictly “greater than” x.\ndef bisect_right[T](\n    x: T, items: Sequence[T], f: Ordering[T] = None\n) -&gt; int:\n    if f is None:\n        f = default_order\n\n    l, r = 0, len(items)\n\n    while l &lt; r:\n        m = (l + r) // 2\n        if f(x, items[m]) &lt; 0:\n            r = m\n        else:\n            l = m + 1\n\n    return l\nIn this variant, we only move the left boundary l forward if x &gt;= items[m], which pushes the search toward the end of a block of identical values.\nSince both functions follow the same halving principle as standard binary search, their performance characteristics are identical:\n\nTime Complexity: \\(O(\\log n)\\), as we halve the search space in every iteration of the while loop.\nSpace Complexity: \\(O(1)\\), as we only maintain two integer indices regardless of the input size.\n\nTo ensure these boundaries are calculated correctly, especially with duplicate elements, we use the following test cases:\nfrom codex.search.binary import bisect_left, bisect_right\n\ndef test_bisection_boundaries():\n    # Sequence with a \"block\" of 2s\n    items = [1, 2, 2, 2, 3]\n\n    # First index where 2 is (or could be)\n    assert bisect_left(2, items) == 1\n\n    # Index after the last 2\n    assert bisect_right(2, items) == 4\n\n    # If element is missing, both return the same insertion point\n    assert bisect_left(1.5, items) == 1\n    assert bisect_right(1.5, items) == 1\n\ndef test_bisection_extremes():\n    items = [1, 2, 3]\n    assert bisect_left(0, items) == 0\n    assert bisect_right(4, items) == 3",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html#binary-search-on-predicates",
    "href": "02_binary.html#binary-search-on-predicates",
    "title": "3  Efficient Search",
    "section": "3.3 Binary Search on Predicates",
    "text": "3.3 Binary Search on Predicates\nThe true power of binary search extends far beyond finding a number in a list. We can generalize the algorithm to find the “boundary” of any monotonic predicate.\nA predicate is monotonic if, once it becomes true for some index , it remains true for all . Formally, if , we can use binary search to find the smallest index such that is true. This is often called “Binary Search on the Answer.”\nInstead of searching through a physical collection of items, we are searching through an abstract decision space.\nfrom typing import Callable\n\ndef find_first(\n    low: int, high: int, p: Callable[[int], bool]\n) -&gt; int | None:\n    \"\"\"\n    Finds the first index in [low, high] for which p(index) is True.\n    Assumes p is monotonic: if p(i) is True, p(i+1) is also True.\n    \"\"\"\n    ans = None\n    l, r = low, high\n\n    while l &lt;= r:\n        m = (l + r) // 2\n        if p(m):\n            ans = m\n            r = m - 1\n        else:\n            l = m + 1\n\n    return ans\nConsider the problem of finding the integer square root of a very large number —that is, the largest integer such that . While we could use math.sqrt, binary search allows us to find this value using only integer arithmetic, which is vital in fields like cryptography or when dealing with arbitrary-precision integers.\nOur predicate is: “Is ?” This is monotonic: if , then is certainly . By finding the first where , we know that is our desired integer square root.\ndef integer_sqrt(n: int) -&gt; int:\n    if n &lt; 0:\n        raise ValueError(\"Square root not defined for negative numbers\")\n    if n &lt; 2:\n        return n\n\n    # Find the first x such that x*x &gt; n\n    first_too_big = find_first(1, n, lambda x: x * x &gt; n)\n\n    return first_too_big - 1\nThis approach reveals a deep connection between searching and optimization. Many problems that ask for a “minimum possible such that is possible” can be solved by binary searching over the value of , provided that the possibility is monotonic relative to .\nWhenever you encounter a problem where a “yes” answer for a value implies a “yes” for all values larger than , you are no longer looking for an item—you are looking for a threshold. Binary search is the most efficient way to discover it.\n\n3.3.1 Verification\nWe can verify this generalized search and its application to the integer square root problem with the following tests.\nfrom codex.search.binary import find_first, integer_sqrt\n\ndef test_find_first():\n    # Predicate: is the number &gt;= 7?\n    nums = [1, 3, 5, 7, 9, 11]\n    # find_first returns the index\n    idx = find_first(0, len(nums) - 1, lambda i: nums[i] &gt;= 7)\n    assert idx == 3\n    assert nums[idx] == 7\n\ndef test_integer_sqrt():\n    assert integer_sqrt(16) == 4\n    assert integer_sqrt(15) == 3\n    assert integer_sqrt(17) == 4\n    assert integer_sqrt(0) == 0\n    assert integer_sqrt(1) == 1\n    assert integer_sqrt(10**20) == 10**10",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "02_binary.html#conclusion",
    "href": "02_binary.html#conclusion",
    "title": "3  Efficient Search",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nSearching is arguably the most important problem in Computer Science. In this first chapter, we have only scratched the surface of this vast field, but in doing so, we have discovered one of the fundamental truths of computation: structure matters–a lot.\nWhen we know nothing about the structure of our problem or the collection of items we are searching through, we have no choice but to rely on exhaustive methods like linear search. In these cases, we must check every single item to determine if it is the one we care about.\nHowever, as soon as we introduce some structure–specifically, some order–the landscape changes completely. Binary search allows us to exploit this structure to find an element as fast as is theoretically possible, reducing our workload from a linear progression to a logarithmic one.\nThis realization that we can trade a bit of organizational effort for a massive gain in search efficiency is the perfect segue for our next chapter. If searching is easier when items are ordered, then we must understand the process of establishing that order. We must talk about sorting.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Efficient Search</span>"
    ]
  },
  {
    "objectID": "03_sort.html",
    "href": "03_sort.html",
    "title": "4  Basic Sorting",
    "section": "",
    "text": "4.1 Selection Sort\nAs we discovered in the previous chapter, structure is the secret ingredient that makes computation efficient. Searching through an unordered collection is a tedious, linear process, but searching through an ordered one is exponentially faster.\nSorting is the process of establishing this order. Formally, we want to take a sequence of items and rearrange them into a new sequence where, for any two indices \\(i\\) and \\(j\\), if \\(i &lt; j\\), then \\(x_i &lt; x_j\\). In this chapter, we explore the most fundamental ways to achieve this, building our intuition from simple observation to a deeper structural analysis of the sorting problem itself.\nThe most intuitive way to sort a list is to think about the destination. If we are building a sorted sequence, the very first element (index 0) must be the minimum element of the entire collection. Once that is in place, the second element (index 1) must be the minimum of everything that remains, and so on.\nIn Selection Sort, we stand at each position in the array and ask: “Which element belongs here?” To answer, we scan the unsorted portion of the list, find the minimum, and swap it into our current position.\nWe implement this by directly searching for the minimum index in each iteration rather than calling a helper function, keeping the logic self-contained. Because we must scan the remaining items for every single position in the list, we perform roughly \\(1 + 2 + 3 + ... (n-1) = n(n-1)/2\\) comparisons, leading to a time complexity of \\(O(n^2)\\).",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#selection-sort",
    "href": "03_sort.html#selection-sort",
    "title": "4  Basic Sorting",
    "section": "",
    "text": "from typing import MutableSequence\nfrom codex.types import Ordering, default_order\n\ndef selection_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    n = len(items)\n    for i in range(n):\n        # Find the index of the minimum element in the unsorted suffix\n        min_idx = i\n        for j in range(i + 1, n):\n            if f(items[j], items[min_idx]) &lt; 0:\n                min_idx = j\n\n        # Swap it into the current position\n        items[i], items[min_idx] = items[min_idx], items[i]",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#insertion-sort",
    "href": "03_sort.html#insertion-sort",
    "title": "4  Basic Sorting",
    "section": "4.2 Insertion Sort",
    "text": "4.2 Insertion Sort\nWe can flip the narrative of Selection Sort. Instead of standing at a position and looking for the right element, we can take an element and look for its right position. This is how most people sort a hand of cards.\nWe assume that everything behind our current position is already sorted. We take the next element and ask: “How far back must I move this element so that the sequence remains sorted?” We shift it backward, swapping it with its predecessor, until it finds its rightful place.\ndef insertion_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    for i in range(1, len(items)):\n        j = i\n        # Move the element backward as long as it is smaller than its predecessor\n        while j &gt; 0 and f(items[j], items[j-1]) &lt; 0:\n            items[j], items[j-1] = items[j-1], items[j]\n            j -= 1\nThe first element is sorted by definition. The second element either stays put or moves before the first. The third moves until it is in the correct spot relative to the first two. In the worst case (a reverse-sorted list), this also results again in \\(O(n^2)\\) operations, but it is remarkably efficient for lists that are already “nearly sorted.”",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#the-geometry-of-inversions",
    "href": "03_sort.html#the-geometry-of-inversions",
    "title": "4  Basic Sorting",
    "section": "4.3 The Geometry of Inversions",
    "text": "4.3 The Geometry of Inversions\nTo understand why these algorithms are all quadratic in complexity, we need to look at the structure of “unsortedness.” Whenever a list in unsorted, is because we can find at least a couple of elements that are out of place. This means some \\(x_i &gt; x_j\\) where \\(i &lt; j\\). We define an inversion as any pair of such items. A sorted list has zero inversions.\nWith this idea in place, we can see sorting as “just” the problem of reducing the number of inversions down to zero. Any algorithm that does progress towards reducing the number of inversions is actually sorting. And a crucial insight is that there can be at most \\(O(n^2)\\) inversions in any sequence of size \\(n\\).\nSelection sort reduces up to \\(n\\) inversions with each swap, that is, all inversions relative to the current minimum element. But every swap requires up to \\(n\\) comparisons, so we get \\(O(n^2)\\) steps. Insertion sort reduces at most one inversion each step, by moving one item forward, thus it will require \\(O(n^2)\\) steps to eliminate that many inversions.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#bubble-sort",
    "href": "03_sort.html#bubble-sort",
    "title": "4  Basic Sorting",
    "section": "4.4 Bubble Sort",
    "text": "4.4 Bubble Sort\nLet’s now build an algorithm based on this idea of eliminating inversions directly. A first guess could be, lets try to find a pair of inverted items and swap them. But we must be careful, if we do indiscrimantely, we might end up fixing one inversion but creating other inversions.\nHowever, a powerful idea that we won’t formally proof is that if a list has any inversions, there must be at least one inversion between two consecutive elements. If we fix these local inversions, we eventually fix them all. And fixing an inversion between consecutive elements cannot create new inversions. This is the heart of Bubble Sort: we repeatedly step through the list and swap adjacent items that are out of order.\ndef bubble_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    n = len(items)\n    for i in range(n):\n        swapped = False\n        for j in range(0, n - i - 1):\n            # If we find a consecutive inversion, fix it\n            if f(items[j+1], items[j]) &lt; 0:\n                items[j], items[j+1] = items[j+1], items[j]\n                swapped = True\n\n        # If no swaps occurred, the list is already sorted\n        if not swapped:\n            break",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#verification",
    "href": "03_sort.html#verification",
    "title": "4  Basic Sorting",
    "section": "4.5 Verification",
    "text": "4.5 Verification\nTo ensure these three fundamental sorting approaches work as intended, we can run them against a set of standard cases.\nimport pytest\nfrom codex.sort.basic import selection_sort, insertion_sort, bubble_sort\n\n@pytest.mark.parametrize(\"sort_fn\", [selection_sort, insertion_sort, bubble_sort])\ndef test_sorting_algorithms(sort_fn):\n    items = [4, 2, 7, 1, 3]\n    sort_fn(items)\n    assert items == [1, 2, 3, 4, 7]\n\n    # Already sorted\n    items = [1, 2, 3]\n    sort_fn(items)\n    assert items == [1, 2, 3]\n\n    # Reverse sorted\n    items = [3, 2, 1]\n    sort_fn(items)\n    assert items == [1, 2, 3]",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "03_sort.html#conclusion",
    "href": "03_sort.html#conclusion",
    "title": "4  Basic Sorting",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nSelection, Insertion, and Bubble sort are all \\(O(n^2)\\) algorithms. The reason is structural: in the worst case, a list of size can have \\(O(n^2)\\) inversions. Since each swap in these algorithms only fixes one inversion at a time–in the best case–we are forced to perform a quadratic number of operations.\nTo break this ceiling and reach the theoretical limit of \\(O(n \\log n)\\), we need to be more clever. We need algorithms that can fix many inversions with a single operation. This “divide and conquer” approach will be the focus of our next chapter: Efficient Sorting.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Basic Sorting</span>"
    ]
  },
  {
    "objectID": "04_sort_fast.html",
    "href": "04_sort_fast.html",
    "title": "5  Efficient Sorting",
    "section": "",
    "text": "5.1 Merge Sort: Intuition through Order\nThis chapter marks our departure from the quadratic barrier. As we established previously, sorting is the process of eliminating inversions. While basic algorithms like Selection or Insertion sort remove inversions somewhat haphazardly or one at a time, efficient sorting relies on a structured, recursive strategy known as Divide and Conquer.\nBy imposing a rigid structure on how we approach these inversions, we can reduce the computational cost from \\(O(n^2)\\) to the theoretical optimum of \\(O(n \\log n)\\).\nTo break the quadratic limit, we must find ways to fix multiple inversions with a single operation. The most effective way to do this is to stop looking at the sequence as a monolithic block and start viewing it as a composition of smaller, more manageable sub-problems.\nThe fundamental insight behind Merge Sort is that it is remarkably easy to combine two sequences that are already sorted. If we have two sorted lists, we can merge them into a single sorted list in linear time by simply comparing the heads of each list and picking the smaller one.\nThe “conquer” part of the algorithm is this linear merge. The “divide” part is the recursive leap: if we don’t have two sorted halves, we simply split our current list in two and call Merge Sort on each piece until we reach the base case—a list of a single element, which is sorted by definition.\nIn Merge Sort, the structural strategy is to fix inversions within each half first. We recursively dive deep into the sequence, sorting smaller and smaller sub-sequences. Only when the sub-sequences are internally free of inversions do we perform the merge step to fix the “global” inversions between the two halves.\nBecause the list is halved at each step, the recursion depth is \\(log(n)\\). Since we perform \\(O(n)\\) work at each level of the tree to merge the results, the total complexity is \\(O(n ĺog n)\\).",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efficient Sorting</span>"
    ]
  },
  {
    "objectID": "04_sort_fast.html#merge-sort-intuition-through-order",
    "href": "04_sort_fast.html#merge-sort-intuition-through-order",
    "title": "5  Efficient Sorting",
    "section": "",
    "text": "from typing import MutableSequence, List\nfrom codex.types import Ordering, default_order\n\ndef merge_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    if len(items) &lt;= 1:\n        return\n\n    mid = len(items) // 2\n    left = items[:mid]\n    right = items[mid:]\n\n    merge_sort(left, f)\n    merge_sort(right, f)\n\n    # Merge the sorted halves back into items\n    i = j = k = 0\n    while i &lt; len(left) and j &lt; len(right):\n        if f(left[i], right[j]) &lt;= 0:\n            items[k] = left[i]\n            i += 1\n        else:\n            items[k] = right[j]\n            j += 1\n        k += 1\n\n    while i &lt; len(left):\n        items[k] = left[i]\n        i += 1\n        k += 1\n\n    while j &lt; len(right):\n        items[k] = right[j]\n        j += 1\n        k += 1",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efficient Sorting</span>"
    ]
  },
  {
    "objectID": "04_sort_fast.html#quick-sort-sorting-by-partitioning",
    "href": "04_sort_fast.html#quick-sort-sorting-by-partitioning",
    "title": "5  Efficient Sorting",
    "section": "5.2 Quick Sort: Sorting by Partitioning",
    "text": "5.2 Quick Sort: Sorting by Partitioning\nWhile Merge Sort is elegant, it requires extra space to store the temporary halves during the merge process. Quick Sort offers a narrative shift: instead of splitting the list blindly in the middle and merging later, we rearrange the items first so that no merge is ever necessary.\nThis is achieved through partitioning. We pick an element called a pivot and rearrange the sequence so that every element smaller than the pivot moved to its left, and every element larger than the pivot moved to its right.\ndef quick_sort[T](\n    items: MutableSequence[T], f: Ordering[T] = None\n) -&gt; None:\n    if f is None:\n        f = default_order\n\n    def _quick_sort(low: int, high: int):\n        if low &lt; high:\n            p = _partition(low, high)\n            _quick_sort(low, p)\n            _quick_sort(p + 1, high)\n\n    def _partition(low: int, high: int) -&gt; int:\n        pivot = items[(low + high) // 2]\n        i = low - 1\n        j = high + 1\n        while True:\n            i += 1\n            while f(items[i], pivot) &lt; 0:\n                i += 1\n            j -= 1\n            while f(items[j], pivot) &gt; 0:\n                j -= 1\n            if i &gt;= j:\n                return j\n            items[i], items[j] = items[j], items[i]\n\n    _quick_sort(0, len(items) - 1)\nThe striking difference here is the order of operations. Quick Sort fixes inversions between both halves first. By partitioning, we ensure that there are zero inversions between the left “half” and the right “half” (no item in the left is greater than an item in the right). Once this global structure is established, we recursively fix the remaining inversions within each half.\nThis approach allows Quick Sort to operate in-place, requiring only \\(O(\\log n)\\) auxiliary space for the recursion stack. While its worst-case is \\(O(n^2)\\), its average-case performance is a highly efficient \\(O(n \\log n)\\).",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efficient Sorting</span>"
    ]
  },
  {
    "objectID": "04_sort_fast.html#verification",
    "href": "04_sort_fast.html#verification",
    "title": "5  Efficient Sorting",
    "section": "5.3 Verification",
    "text": "5.3 Verification\nWe can verify both algorithms using the same test suite we established for basic sorting.\nimport pytest\nfrom codex.sort.efficient import merge_sort, quick_sort\n\n@pytest.mark.parametrize(\"sort_fn\", [merge_sort, quick_sort])\ndef test_efficient_sorting(sort_fn):\n    items = [4, 2, 7, 1, 3]\n    sort_fn(items)\n    assert items == [1, 2, 3, 4, 7]\n\n    # Handle duplicates and edge cases\n    items = [2, 1, 2, 1]\n    sort_fn(items)\n    assert items == [1, 1, 2, 2]",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efficient Sorting</span>"
    ]
  },
  {
    "objectID": "04_sort_fast.html#conclusion",
    "href": "04_sort_fast.html#conclusion",
    "title": "5  Efficient Sorting",
    "section": "5.4 Conclusion",
    "text": "5.4 Conclusion\nBoth Merge Sort and Quick Sort reaffirm our central theme: structure matters. By moving away from the unstructured swaps of Bubble Sort and adopting a rigorous divide-and-conquer strategy, we change how we interact with the geometry of inversions.\nWhether we fix local inversions first (Merge Sort) or global ones first (Quick Sort), the result is a massive leap in efficiency. We have traded the simplicity of a double-loop for the power of recursion and structural partitioning.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efficient Sorting</span>"
    ]
  },
  {
    "objectID": "05_rankings.html",
    "href": "05_rankings.html",
    "title": "6  Rankings and Selection",
    "section": "",
    "text": "6.1 Quick Select: Selection via Partitioning\nThis chapter addresses the problem of selecting the -th smallest element in a sequence—a task that lies at the heart of calculating medians, percentiles, and other order statistics. While one could simply sort the entire collection in time and pick the element at index , we can do better by leveraging the same structural insights we gained from Quick Sort.\nFinding the “rank” of an element—its position in a sorted version of the collection—is a fundamental operation in data analysis. The most common case is finding the median, the element that splits a set into two equal halves. By generalizing this, we look for the -th order statistic: the value that is greater than or equal to exactly elements.\nThe core of Quick Sort was the partition operation, which organized elements around a pivot. In Quick Sort, we recursively processed both sides of the partition. However, for selection, we only care about the side that contains our target index .\nThis leads to Quick Select. Because we discard one-half of the search space at every step, we aren’t performing a “divide and conquer” so much as a “prune and search” strategy.\nThe probabilistic analysis of Quick Select is striking. In the worst case—where we consistently pick the worst possible pivot—the complexity is still \\(O(n^2)\\). However, on average, the size of the search space follows a geometric series which converges to \\(O(n)\\). This means that on average, Quick Select finds the \\(k\\)-th order statistic in \\(O(n)\\) time.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rankings and Selection</span>"
    ]
  },
  {
    "objectID": "05_rankings.html#quick-select-selection-via-partitioning",
    "href": "05_rankings.html#quick-select-selection-via-partitioning",
    "title": "6  Rankings and Selection",
    "section": "",
    "text": "import random\nfrom typing import MutableSequence\nfrom codex.types import Ordering, default_order\n\ndef quick_select[T](\n    items: MutableSequence[T], k: int, f: Ordering[T] = None\n) -&gt; T:\n    if f is None:\n        f = default_order\n\n    if not 0 &lt;= k &lt; len(items):\n        raise IndexError(\"Rank k is out of bounds\")\n\n    return _select(items, 0, len(items) - 1, k, f)\n\ndef _select[T](\n    items: MutableSequence[T], low: int, high: int, k: int, f: Ordering[T]\n) -&gt; T:\n    if low == high:\n        return items[low]\n\n    # Randomized pivot selection to ensure good average performance\n    pivot_idx = random.randint(low, high)\n    items[pivot_idx], items[high] = items[high], items[pivot_idx]\n\n    p = _partition(items, low, high, f)\n\n    if k == p:\n        return items[p]\n    elif k &lt; p:\n        return _select(items, low, p - 1, k, f)\n    else:\n        return _select(items, p + 1, high, k, f)\n\ndef _partition[T](\n    items: MutableSequence[T], low: int, high: int, f: Ordering[T]\n) -&gt; int:\n    pivot = items[high]\n    i = low\n    for j in range(low, high):\n        if f(items[j], pivot) &lt;= 0:\n            items[i], items[j] = items[j], items[i]\n            i += 1\n    items[i], items[high] = items[high], items[i]\n    return i",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rankings and Selection</span>"
    ]
  },
  {
    "objectID": "05_rankings.html#median-of-medians-deterministic-selection",
    "href": "05_rankings.html#median-of-medians-deterministic-selection",
    "title": "6  Rankings and Selection",
    "section": "6.2 Median of Medians: Deterministic Selection",
    "text": "6.2 Median of Medians: Deterministic Selection\nWhile randomization is practically robust, we can achieve a guaranteed worst-case time using a clever, ad hoc approach to pivot selection known as the Median of Medians algorithm.\nThe goal is to find a pivot that is guaranteed to be “good enough”—meaning it is not too close to either end of the sorted sequence. We do this by:\n\nDividing the list into groups of five.\nFinding the median of each small group.\nRecursively finding the median of these medians.\n\nThis “median of medians” is then used as the pivot for a standard partition.\ndef median_of_medians[T](\n    items: MutableSequence[T], k: int, f: Ordering[T] = None\n) -&gt; T:\n    if f is None:\n        f = default_order\n\n    def _get_pivot(sub_items: MutableSequence[T]) -&gt; T:\n        if len(sub_items) &lt;= 5:\n            return sorted(sub_items, key=lambda x: x)[len(sub_items) // 2]\n\n        chunks = [sub_items[i:i + 5] for i in range(0, len(sub_items), 5)]\n        medians = [sorted(c, key=lambda x: x)[len(c) // 2] for c in chunks]\n        return _get_pivot(medians)\n\n    # TODO: Use the median of medians to partition and select\n    # (Implementation follows standard select logic using the calculated pivot)\n    # ...\nThe structural beauty of this algorithm lies in the constant 5. It is the smallest odd number that ensures the recursive step prunes enough of the search space to maintain a linear recurrence.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rankings and Selection</span>"
    ]
  },
  {
    "objectID": "05_rankings.html#verification",
    "href": "05_rankings.html#verification",
    "title": "6  Rankings and Selection",
    "section": "6.3 Verification",
    "text": "6.3 Verification\nTo ensure our selection logic holds, we test it by finding various ranks in both random and edge-case sequences.\nimport pytest\nfrom codex.search.rank import quick_select\n\ndef test_selection():\n    items = [3, 1, 2, 4, 0]\n    # Finding the median (rank 2)\n    assert quick_select(items[:], 2) == 2\n    # Finding the minimum (rank 0)\n    assert quick_select(items[:], 0) == 0\n    # Finding the maximum (rank 4)\n    assert quick_select(items[:], 4) == 4\n\ndef test_duplicates():\n    items = [1, 2, 1, 2, 1]\n    assert quick_select(items, 0) == 1\n    assert quick_select(items, 4) == 2",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rankings and Selection</span>"
    ]
  },
  {
    "objectID": "05_rankings.html#conclusion",
    "href": "05_rankings.html#conclusion",
    "title": "6  Rankings and Selection",
    "section": "6.4 Conclusion",
    "text": "6.4 Conclusion\nWith the implementation of selection algorithms, we have completed our initial survey of searching and sorting. The progression from \\(O(n)\\) linear search to \\(O(n \\log n)\\) sorting, and finally back to \\(O(n)\\) for selection, demonstrates the power of structured thinking. By understanding how to manipulate inversions and search spaces, we can find specific needles in increasingly large haystacks with mathematical precision.\nBefore moving on, we will briefly touch on one more subject: linear sorting. We will see how digging further down into specific structural constraints of the of the input data we can furhter refine our algorithms and make them even faster.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Rankings and Selection</span>"
    ]
  },
  {
    "objectID": "06_linear_sort.html",
    "href": "06_linear_sort.html",
    "title": "7  Linear Time Sorting",
    "section": "",
    "text": "7.1 Counting Sort\nThis chapter concludes our exploration of sorting by demonstrating that the limit we established is not a universal law, but a specific constraint of comparison-based algorithms. If we possess even deeper knowledge about the structure of our data—specifically that it consists of discrete values within a known range—we can bypass comparisons entirely and achieve true linear time performance.\nIn the previous chapters, we operated under the assumption that the only way to gain information about our items was to compare them. However, when our data has a predictable, bounded structure, we can use the values themselves as indices. This shifts the problem from “which is bigger?” to “how many of each value do we have?”.\nCounting Sort is the purest expression of this idea. If we know that every element in a sequence is an integer in the range , we can simply count the occurrences of each integer. By calculating the cumulative sum of these counts, we determine the exact position each element should occupy in the final sorted array.\nCounting Sort performs exactly two passes over the input and one pass over the range \\(O(k)\\). This results in a time complexity of \\(O(n+k)\\). When \\(k = O(n)\\), the algorithm is strictly linear. The cost, however, is space complexity: we require an auxiliary array of size \\(k\\). If \\(k\\) is significantly larger than \\(n\\), this becomes impractical.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Sorting</span>"
    ]
  },
  {
    "objectID": "06_linear_sort.html#counting-sort",
    "href": "06_linear_sort.html#counting-sort",
    "title": "7  Linear Time Sorting",
    "section": "",
    "text": "from typing import MutableSequence, Callable, List\nfrom codex.types import Ordering, default_order\n\ndef counting_sort(\n    items: MutableSequence[int], k: int\n) -&gt; List[int]:\n    \"\"\"\n    Sorts a sequence of integers in the range [0, k].\n    This implementation is stable.\n    \"\"\"\n    counts = [0] * (k + 1)\n    for x in items:\n        counts[x] += 1\n\n    # Transform counts into starting indices\n    for i in range(1, k + 1):\n        counts[i] += counts[i - 1]\n\n    output = [0] * len(items)\n    # Iterate backwards to maintain stability\n    for x in reversed(items):\n        counts[x] -= 1\n        output[counts[x]] = x\n\n    return output",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Sorting</span>"
    ]
  },
  {
    "objectID": "06_linear_sort.html#radix-sort-sorting-by-digits",
    "href": "06_linear_sort.html#radix-sort-sorting-by-digits",
    "title": "7  Linear Time Sorting",
    "section": "7.2 Radix Sort: Sorting by Digits",
    "text": "7.2 Radix Sort: Sorting by Digits\nTo handle larger ranges without massive memory overhead, we use Radix Sort. The intuition here is to view each number (or string) as a sequence of “digits.” We sort the collection multiple times, once for each digit position, starting from the least significant digit (LSD).\nCrucially, each pass must be a stable sort. By using Counting Sort as the stable subroutine for each digit, we can sort numbers in any range by breaking them into digits.\ndef radix_sort(items: MutableSequence[int], base: int = 10) -&gt; List[int]:\n    if not items:\n        return items\n\n    max_val = max(items)\n    exp = 1\n    output = list(items)\n\n    while max_val // exp &gt; 0:\n        output = _counting_sort_by_digit(output, exp, base)\n        exp *= base\n\n    return output\n\ndef _counting_sort_by_digit(items: List[int], exp: int, base: int) -&gt; List[int]:\n    counts = [0] * base\n    for x in items:\n        digit = (x // exp) % base\n        counts[digit] += 1\n\n    for i in range(1, base):\n        counts[i] += counts[i - 1]\n\n    res = [0] * len(items)\n    for x in reversed(items):\n        digit = (x // exp) % base\n        counts[digit] -= 1\n        res[counts[digit]] = x\n    return res\nRadix Sort runs in $O(d(n+k)) time, where \\(d\\) is the number of digits and \\(k\\) is the base. Because \\(d\\) is constant for a fixed word size (e.g., 32-bit or 64-bit integers), the algorithm remains linear relative to \\(n\\). This is the preferred method for sorting large sets of integers or fixed-length strings where memory is constrained compared to the range of values.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Sorting</span>"
    ]
  },
  {
    "objectID": "06_linear_sort.html#verification",
    "href": "06_linear_sort.html#verification",
    "title": "7  Linear Time Sorting",
    "section": "7.3 Verification",
    "text": "7.3 Verification\nWe verify these linear approaches by testing them against various integer ranges and sequences.\nimport pytest\nfrom codex.sort.linear import counting_sort, radix_sort\n\ndef test_counting_sort():\n    items = [4, 1, 3, 4, 3]\n    # Range is [0, 4]\n    assert counting_sort(items, 4) == [1, 3, 3, 4, 4]\n\ndef test_radix_sort():\n    items = [170, 45, 75, 90, 802, 24, 2, 66]\n    expected = [2, 24, 45, 66, 75, 90, 170, 802]\n    assert radix_sort(items) == expected",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Sorting</span>"
    ]
  },
  {
    "objectID": "06_linear_sort.html#conclusions",
    "href": "06_linear_sort.html#conclusions",
    "title": "7  Linear Time Sorting",
    "section": "7.4 Conclusions",
    "text": "7.4 Conclusions\nThe algorithms in this chapter serve as a powerful reminder that theoretical limits are often tied to specific constraints. By moving away from the “black box” of comparison—where we know nothing about items except their relative order—to a model where we exploit the internal structure of keys, we successfully broke the \\(O(n \\log n)\\) barrier.\nThrough Counting Sort, we saw how integers can be used directly as indices to map values to their final positions in \\(O(n + k)\\) time. With Radix Sort, we extended this principle to larger ranges by decomposing keys into digits, maintaining linearity through multiple stable passes.\nHowever, this efficiency is not a “free lunch.” We have traded mathematical generality for physical memory, as these algorithms require auxiliary space proportional to the range of values (\\(O(k)\\)) or the base used for digits. Furthermore, they are only applicable to discrete, bounded data types.\nThis chapter concludes our survey of sorting by reaffirming the Codex’s central thesis: the more you know about the structure of your data, the more effectively you can master its complexity. Having mastered the logic of search and order, we are now ready to explore how these abstract processes are grounded in the physical topology of memory in Part II: Fundamental Data Structures.",
    "crumbs": [
      "Searching and Sorting",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Linear Time Sorting</span>"
    ]
  },
  {
    "objectID": "lessons_1.html",
    "href": "lessons_1.html",
    "title": "Lessons from Part I",
    "section": "",
    "text": "As we conclude our exploration of searching and sorting, we move away from specific implementations to look at the meta-principles of algorithm design. The algorithms we have studied are more than just tools; they are demonstrations of how a programmer can negotiate with the laws of logic to extract performance.\nThe most profound lesson of this part is that structure matters. In an unstructured sequence, finding an item is a linear struggle against entropy. By imposing order, we transform the search space into a hierarchy of information where every comparison halves our remaining work.\nThis theme repeated in our transition from basic to efficient sorting. By recognizing the “Geometry of Inversions,” we moved from haphazardly swapping neighbors to structured partitioning and merging. The lesson is universal: to make a process efficient, you must first organize the environment in which it operates.\nWe discovered that the “best” algorithm often depends on how much we are willing to assume about our data. The barrier for sorting is only a law for comparison-based logic. By imposing stricter conditions—such as requiring inputs to be integers in a known range—we unlocked Counting Sort and Radix Sort, achieving true linear time.\nIn algorithm design, constraints are not limitations; they are opportunities. The more you know about the structure of your input, the more aggressively you can optimize your solution.\nThroughout this part, recursion has been our primary tool for managing complexity. Whether in the binary halving of a search space or the divide-and-conquer strategy of Merge Sort, recursion allows us to solve a problem by defining what it means to be “partially solved.” It is the mathematical expression of delegation, allowing us to focus on the logic of a single step while the structure of the call stack handles the global coordination.\nFinally, we introduced randomization as a strategic weapon. In Quick Select, we saw that while a deterministic worst-case can be expensive, a randomized approach can offer performance with a probability so high it effectively becomes a certainty. This represents a pragmatic shift in computational thinking: sometimes, the most “rational” path is to embrace the roll of the dice to avoid the pathological cases that break deterministic logic.",
    "crumbs": [
      "Searching and Sorting",
      "Lessons from Part I"
    ]
  },
  {
    "objectID": "part2.html",
    "href": "part2.html",
    "title": "Fundamental Data Structures",
    "section": "",
    "text": "What We Will Explore\nIn the first part of this Codex, we treated data largely as an abstract sequence—a collection of items that we could index, compare, and rearrange at will. We discovered that by imposing logical order, we could achieve remarkable algorithmic efficiency. However, algorithms do not exist in a vacuum; they must inhabit the physical reality of a computer’s memory.\nIn this part, we shift our focus from the logic of the process to the topology of the data. Here, we explore how the way we organize information in memory—whether as a contiguous block or a web of pointers—determines the physical limits of the algorithms we write.\nIn this part, we move beyond the high-level abstractions provided by Python’s built-in types to implement the essential structures that form the foundation of all modern software systems:",
    "crumbs": [
      "Fundamental Data Structures"
    ]
  },
  {
    "objectID": "part2.html#what-we-will-explore",
    "href": "part2.html#what-we-will-explore",
    "title": "Fundamental Data Structures",
    "section": "",
    "text": "Memory and Sequences: We begin by examining the trade-offs between contiguous memory (arrays) and linked structures, understanding why the physical layout of data is the primary driver of performance.\nLinked Lists: We implement the most fundamental non-contiguous structure, exploring how pointers allow for dynamic growth and efficient insertions at the cost of random access.\nStacks and Queues: We build the primary abstractions for managing order and flow, implementing LIFO (Last-In, First-Out) and FIFO (First-In, First-Out) behaviors that are critical for everything from expression evaluation to task scheduling.\nHashing and Hash Tables: We investigate the “magic” of constant-time access, learning how to bridge the gap between an arbitrary value and its physical location in memory through hash functions and collision resolution strategies.",
    "crumbs": [
      "Fundamental Data Structures"
    ]
  },
  {
    "objectID": "part2.html#what-you-will-learn",
    "href": "part2.html#what-you-will-learn",
    "title": "Fundamental Data Structures",
    "section": "What You Will Learn",
    "text": "What You Will Learn\nBy the end of this part, you will have moved from a theoretical understanding of algorithms to a practical mastery of their physical containers. Specifically, you will learn:\n\nThe Physicality of Data: Why the distinction between a pointer and an index is the most important decision in low-level system design.\nTrade-off Analysis: How to weigh the benefits of fast insertion against the necessity of fast retrieval, moving beyond simple Big O notation to consider cache locality and memory overhead.\nAbstractions and Protocols: How to use Python’s modern type system and protocols to implement these structures in a way that is generic, reusable, and mathematically sound.\nBuilding Blocks: How these simple structures—lists, stacks, and queues—serve as the indispensable components for the more complex trees and graphs we will encounter later in the Codex.\n\nIn the same way that a builder must understand the properties of wood, steel, and stone, a computer scientist must understand the properties of data structures. We are moving from the design of the strategy to the selection of the material.",
    "crumbs": [
      "Fundamental Data Structures"
    ]
  },
  {
    "objectID": "appendix-python.html",
    "href": "appendix-python.html",
    "title": "Appendix A — A Python Primer",
    "section": "",
    "text": "A.1 Variables and Basic Types\nThis appendix provides a concise overview of the Python 3.13 features and syntax used throughout The Algorithm Codex. While this book is not an introductory programming text, this primer serves as a reference for the specific idioms and modern type-system features that enable our clean, algorithmic implementations.\nPython is a high-level, interpreted language that prioritizes readability and expressiveness. In this Codex, we treat Python as a executable notation for algorithms, leveraging its modern type system to ensure our code is both correct and self-documenting.\nVariables in Python are names that point to objects in memory. Unlike many lower-level languages, variables do not have fixed types; however, the objects they point to do.\nIn the Codex, we always provide type hints for variables in global or class scopes to maintain clarity.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-python.html#variables-and-basic-types",
    "href": "appendix-python.html#variables-and-basic-types",
    "title": "Appendix A — A Python Primer",
    "section": "",
    "text": "# Integers and Floats\nn: int = 42\npi: float = 3.14159\n\n# Strings and Booleans\nname: str = \"Codex\"\nis_active: bool = True",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-python.html#expressions-and-statements",
    "href": "appendix-python.html#expressions-and-statements",
    "title": "Appendix A — A Python Primer",
    "section": "A.2 Expressions and Statements",
    "text": "A.2 Expressions and Statements\nAn expression is a piece of code that evaluates to a value (e.g., 2 + 2), while a statement is an instruction that performs an action (e.g., an assignment or a function call).\nPython supports standard arithmetic operators (+, -, *, /) and a specific operator for integer division (//), which we use extensively in binary search and partitioning algorithms to find middle indices.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-python.html#control-flow-loops-and-conditionals",
    "href": "appendix-python.html#control-flow-loops-and-conditionals",
    "title": "Appendix A — A Python Primer",
    "section": "A.3 Control Flow: Loops and Conditionals",
    "text": "A.3 Control Flow: Loops and Conditionals\nWe rely on two primary looping constructs to traverse data structures: for loops for iterating over sequences and while loops for processes that continue until a specific logical condition is met.\n# Iterating over a sequence\nfor item in [1, 2, 3]:\n    print(item)\n\n# Conditional logic\nif n &gt; 0:\n    # Do something\nelif n &lt; 0:\n    # Do something else\nelse:\n    # Default case\nIn our implementations of Selection Sort and Bubble Sort, we use range() to generate indices for controlled iteration.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-python.html#functions",
    "href": "appendix-python.html#functions",
    "title": "Appendix A — A Python Primer",
    "section": "A.4 Functions",
    "text": "A.4 Functions\nFunctions are the primary unit of work in this book. We prefer “pure” functions that take inputs, perform a transformation, and return a result without side effects.\ndef square(x: int) -&gt; int:\n    return x * x",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-python.html#classes-and-objects",
    "href": "appendix-python.html#classes-and-objects",
    "title": "Appendix A — A Python Primer",
    "section": "A.5 Classes and Objects",
    "text": "A.5 Classes and Objects\nWhile the Codex favors a functional style, we use classes as simple data containers or to implement specific protocols. Python 3.13 allows for clean class definitions that integrate seamlessly with the type system.\nclass Point:\n    def __init__(self, x: float, y: float):\n        self.x = x\n        self.y = y",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-python.html#type-annotations-and-modern-generics",
    "href": "appendix-python.html#type-annotations-and-modern-generics",
    "title": "Appendix A — A Python Primer",
    "section": "A.6 Type Annotations and Modern Generics",
    "text": "A.6 Type Annotations and Modern Generics\nThe most important feature of the Codex’s coding style is the use of PEP 695 generics, introduced in Python 3.12 and refined in 3.13. This allows us to write algorithms that work for any data type T while maintaining full type safety.\nInstead of older TypeVar syntax, we use the elegant bracket notation:\n# A generic function that works for any type T\ndef identity[T](value: T) -&gt; T:\n    return value\n\n# Using the 'type' alias for complex definitions\ntype Ordering[T] = Callable[[T, T], int]\nThis syntax is used throughout our searching and sorting implementations to ensure that an algorithm designed for integers works just as correctly for strings or custom objects, provided they satisfy the required protocols.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-python.html#the-sequence-protocol",
    "href": "appendix-python.html#the-sequence-protocol",
    "title": "Appendix A — A Python Primer",
    "section": "A.7 The Sequence Protocol",
    "text": "A.7 The Sequence Protocol\nIn almost every chapter, we use Sequence or MutableSequence from the typing module. These are Protocols–they define what a type can do (like being indexed or having a length) rather than what it is. By using Sequence[T] instead of list[T], our algorithms remain generic enough to work with lists, tuples, or custom array-like structures.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Python Primer</span>"
    ]
  },
  {
    "objectID": "appendix-notation.html",
    "href": "appendix-notation.html",
    "title": "Appendix B — Asymptotic Notation and Analysis",
    "section": "",
    "text": "B.1 The Big-O Family\nThis appendix provides a formal grounding for the “scaling intuition” introduced in the foundations of the Codex. In the study of algorithms, we are less concerned with exact cycle counts and more with asymptotic behavior: how the resource requirements of an algorithm grow as the input size \\(n\\) approaches infinity.\nTo analyze algorithms rigorously, we use a set of mathematical notations that allow us to ignore constant factors and lower-order terms, focusing instead on the rate of growth.\nLet \\(f(n)\\) and \\(g(n)\\) be functions mapping natural numbers to non-negative real numbers.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Asymptotic Notation and Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-notation.html#the-big-o-family",
    "href": "appendix-notation.html#the-big-o-family",
    "title": "Appendix B — Asymptotic Notation and Analysis",
    "section": "",
    "text": "B.1.1 Big-\\(O\\) (Upper Bound)\nWe say that \\(f(n) = O(g(n))\\) if there exist positive constants \\(c\\) and \\(n_0\\) such that:\n\\[0 \\leq f(n) \\leq c \\cdot g(n) \\text{ for all } n \\geq n_0\\]\nThis notation provides an asymptotic upper bound. If an algorithm is \\(O(n^2)\\), it will perform no worse than quadratic time for large \\(n\\).\n\n\nB.1.2 Big-\\(\\Omega\\) (Lower Bound)\nWe say that \\(f(n) = \\Omega(g(n))\\) if there exist positive constants \\(c\\) and \\(n_0\\) such that:\n\\[0 \\leq c \\cdot g(n) \\leq f(n) \\text{ for all } n \\geq n_0\\]\n\\(\\Omega\\) provides a lower bound, representing the minimum amount of work an algorithm must perform.\n\n\nB.1.3 Big-\\(\\Theta\\) (Tight Bound)\nWe say that \\(f(n) = \\Theta(g(n))\\) if \\(f(n) = O(g(n))\\) and \\(f(n) = \\Omega(g(n))\\). This means there exist positive constants \\(c_1, c_2,\\) and \\(n_0\\) such that:\n\\[c_1 \\cdot g(n) \\leq f(n) \\leq c_2 \\cdot g(n) \\text{ for all } n \\geq n_0\\]\n\\(\\Theta\\) is the most descriptive notation, as it tells us the algorithm grows exactly like \\(g(n)\\).\n\n\nB.1.4 Little-\\(o\\) and Little-\\(\\omega\\) (Strict Bounds)\n\n\\(f(n) = o(g(n))\\): The growth of \\(f(n)\\) is strictly less than \\(g(n)\\). Formally, \\(\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\\).\n\\(f(n) = \\omega(g(n))\\): The growth of \\(f(n)\\) is strictly greater than \\(g(n)\\). Formally, \\(\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Asymptotic Notation and Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-notation.html#the-master-theorem",
    "href": "appendix-notation.html#the-master-theorem",
    "title": "Appendix B — Asymptotic Notation and Analysis",
    "section": "B.2 The Master Theorem",
    "text": "B.2 The Master Theorem\nMany of the most efficient algorithms in this book (like Merge Sort) use a divide-and-conquer strategy. Their complexity is often expressed as a recurrence of the form:\n\\[T(n) = aT(n/b) + f(n)\\]\nwhere \\(a \\geq 1\\) is the number of subproblems, \\(b &gt; 1\\) is the factor by which the subproblem size is reduced, and \\(f(n)\\) is the cost of work done outside the recursive calls. The Master Theorem provides a “cookbook” solution for such recurrences:\n\nIf \\(f(n) = O(n^{\\log_b a - \\epsilon})\\) for some \\(\\epsilon &gt; 0\\), then \\(T(n) = \\Theta(n^{\\log_b a})\\). (Work is dominated by the leaves).\nIf \\(f(n) = \\Theta(n^{\\log_b a})\\), then \\(T(n) = \\Theta(n^{\\log_b a} \\log n)\\). (Work is balanced across levels).\nIf \\(f(n) = \\Omega(n^{\\log_b a + \\epsilon})\\) and satisfies the regularity condition (\\(af(n/b) \\leq cf(n)\\)), then \\(T(n) = \\Theta(f(n))\\). (Work is dominated by the root).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Asymptotic Notation and Analysis</span>"
    ]
  },
  {
    "objectID": "appendix-notation.html#the-hierarchy-of-growth",
    "href": "appendix-notation.html#the-hierarchy-of-growth",
    "title": "Appendix B — Asymptotic Notation and Analysis",
    "section": "B.3 The Hierarchy of Growth",
    "text": "B.3 The Hierarchy of Growth\nUnderstanding the relative order of complexity functions is essential for selecting the right algorithm for a given problem size. Below is the standard hierarchy from slowest to fastest growth:\n\n\\(O(1)\\) — Constant: Accessing an array element, simple arithmetic.\n\\(O(\\log n)\\) — Logarithmic: Binary search. The “gold standard.”\n\\(O(n)\\) — Linear: Sequential scan.\n\\(O(n \\log n)\\) — Linearithmic: Optimal comparison-based sorting (Merge Sort).\n\\(O(n^2)\\) — Quadratic: Nested loops (Bubble Sort).\n\\(O(n^k)\\) — Polynomial: General algorithmic complexity.\n\\(O(2^n)\\) — Exponential: Exhaustive search (Traveling Salesperson).\n\\(O(n!)\\) — Factorial: Permutations of a set.\n\nAs \\(n\\) grows, the gaps between these classes become astronomical. While a \\(O(n^2)\\) algorithm might be acceptable for \\(n=1,000\\), it becomes entirely impractical for \\(n=1,000,000\\), where an \\(O(n \\log n)\\) or \\(O(n)\\) solution remains trivial for modern hardware.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Asymptotic Notation and Analysis</span>"
    ]
  }
]